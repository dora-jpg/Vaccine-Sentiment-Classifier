{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Artificial Intelligence II - Homework 4\n# Question 3","metadata":{"id":"qLAaIaAE9z4D"}},{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"id":"dWW0-97QvETZ","outputId":"030d359a-3583-4ae9-f0a8-64453d8c2194","execution":{"iopub.status.busy":"2022-03-02T01:02:53.688803Z","iopub.execute_input":"2022-03-02T01:02:53.689179Z","iopub.status.idle":"2022-03-02T01:02:53.710568Z","shell.execute_reply.started":"2022-03-02T01:02:53.689085Z","shell.execute_reply":"2022-03-02T01:02:53.709747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PATH = '/content/drive/MyDrive/Colab Notebooks/Artificial Intelligence II/bert/'","metadata":{"id":"dDtBH8CFwHAt","execution":{"iopub.status.busy":"2022-03-02T01:02:53.712029Z","iopub.execute_input":"2022-03-02T01:02:53.712783Z","iopub.status.idle":"2022-03-02T01:02:53.716848Z","shell.execute_reply.started":"2022-03-02T01:02:53.71274Z","shell.execute_reply":"2022-03-02T01:02:53.715966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Libraries and Read Datasets","metadata":{"id":"7g50JWXbB2ze"}},{"cell_type":"markdown","source":"Import libraries that will be used in this notebook, define a seeding function and set device to cuda if available.\n","metadata":{"id":"rIUe14-z7O9q"}},{"cell_type":"code","source":"# Imports\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport os\n\nimport numpy as np\nfrom numpy import unravel_index\nimport pandas as pd\nimport math\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport sklearn.metrics\nimport seaborn as sns\nimport random\nimport sys\nfrom IPython.display import Image\nimport time\n\n# for text preprocessing\nimport re\nimport string\n\n!CUBLAS_WORKSPACE_CONFIG=:4096:2 # for cuda deterministic behavior\n\n######### BERT ############\n# first install transformers from hugging face\n!pip install transformers\n\n# imports\nfrom transformers import BertTokenizer, BertForQuestionAnswering\n\n# dataloaders \nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\ndef set_seed(seed = 1234):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.enabled = False\n    torch.backends.cudnn.benchmark = False\n    # torch.use_deterministic_algorithms(False)\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed()\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint('Working on:', device)","metadata":{"id":"A7j9F1YTMFuE","outputId":"2df5236b-1df4-4ce0-e313-da4cab2579e5","execution":{"iopub.status.busy":"2022-03-13T12:09:50.354574Z","iopub.execute_input":"2022-03-13T12:09:50.354844Z","iopub.status.idle":"2022-03-13T12:09:58.370733Z","shell.execute_reply.started":"2022-03-13T12:09:50.354814Z","shell.execute_reply":"2022-03-13T12:09:58.369916Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.15.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.62.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.2.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.20.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.26.0)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.47)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.4.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\nRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.6)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.6.0)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.7)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.3)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\nWorking on: cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Convert trivia QA dataset to SQuAD format","metadata":{}},{"cell_type":"code","source":"# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\").\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n# Script based on https://github.com/mandarjoshi90/triviaqa/blob/master/utils/convert_to_squad_format.py\n# We include functions that are modified from https://github.com/mandarjoshi90/triviaqa/tree/master/utils\n# cite: https://github.com/mandarjoshi90/triviaqa/\n\nimport os\nimport argparse\nimport json\nimport nltk\n# from utils.convert_to_squad_format import get_qad_triples\ndef add_triple_data(datum, page, domain):\n    qad = {'Source': domain}\n    for key in ['QuestionId', 'Question', 'Answer']:\n        qad[key] = datum[key]\n    for key in page:\n        qad[key] = page[key]\n    return qad\n\n\ndef get_qad_triples(data):\n    qad_triples = []\n    for datum in data['Data']:\n        for key in ['EntityPages', 'SearchResults']:\n            for page in datum.get(key, []):\n                qad = add_triple_data(datum, page, key)\n                qad_triples.append(qad)\n    return qad_triples\n\n# from utils.utils import get_file_contents\n\ndef get_file_contents(filename, encoding='utf-8'):\n    with open(filename, encoding=encoding) as f:\n        content = f.read()\n    return content\n\n# from utils.dataset_utils import read_triviaqa_data, get_question_doc_string\n\ndef read_clean_part(datum):\n    for key in ['EntityPages', 'SearchResults']:\n        new_page_list = []\n        for page in datum.get(key, []):\n            if page['DocPartOfVerifiedEval']:\n                new_page_list.append(page)\n        datum[key] = new_page_list\n    assert len(datum['EntityPages']) + len(datum['SearchResults']) > 0\n    return datum\n\ndef read_json(filename, encoding='utf-8'):\n    contents = get_file_contents(filename, encoding=encoding)\n    return json.loads(contents)\n\ndef read_triviaqa_data(qajson):\n    data = read_json(qajson)\n    # read only documents and questions that are a part of clean data set\n    if data['VerifiedEval']:\n        clean_data = []\n        for datum in data['Data']:\n            if datum['QuestionPartOfVerifiedEval']:\n                if data['Domain'] == 'Web':\n                    datum = read_clean_part(datum)\n                clean_data.append(datum)\n        data['Data'] = clean_data\n    return data\n\n\ndef get_question_doc_string(qid, doc_name):\n    return '{}--{}'.format(qid, doc_name)\n#-------------------------------------------------------\n\n\n\ndef answer_index_in_document(answer, document):\n    answer_list = answer['Aliases'] + answer['NormalizedAliases']\n    for answer_string_in_doc in answer_list:\n        index = document.find(answer_string_in_doc)\n        if index != -1:\n            return answer_string_in_doc, index\n    return answer['NormalizedValue'], -1\n\n\ndef select_relevant_portion(text):\n    paras = text.split('\\n')\n    selected = []\n    done = False\n    for para in paras:\n        sents = sent_tokenize.tokenize(para)\n        for sent in sents:\n            words = nltk.word_tokenize(sent)\n            for word in words:\n                selected.append(word)\n                if len(selected) >= 800:\n                    done = True\n                    break\n            if done:\n                break\n        if done:\n            break\n        selected.append('\\n')\n    st = ' '.join(selected).strip()\n    return st\n\n\ndef triviaqa_to_squad_format(triviaqa_file, data_dir, output_file):\n    triviaqa_json = read_triviaqa_data(triviaqa_file)\n    qad_triples = get_qad_triples(triviaqa_json)\n\n    data = []\n\n    for triviaqa_example in qad_triples:\n        question_text = triviaqa_example['Question']\n        text = get_file_contents(os.path.join(data_dir, triviaqa_example['Filename']), encoding='utf-8')\n        context = select_relevant_portion(text)\n\n        para = {'context': context, 'qas': [{'question': question_text, 'answers': []}]}\n        data.append({'paragraphs': [para]})\n        qa = para['qas'][0]\n        qa['id'] = get_question_doc_string(triviaqa_example['QuestionId'], triviaqa_example['Filename'])\n        qa['is_impossible'] = True\n        ans_string, index = answer_index_in_document(triviaqa_example['Answer'], context)\n\n        if index != -1:\n            qa['answers'].append({'text': ans_string, 'answer_start': index})\n            qa['is_impossible'] = False\n\n    triviaqa_as_squad = {'data': data, 'version': '2.0'}\n\n    with open(output_file, 'w', encoding='utf-8') as outfile:\n        json.dump(triviaqa_as_squad, outfile, indent=2, sort_keys=True, ensure_ascii=False)\n\n\n\nsent_tokenize = nltk.data.load('tokenizers/punkt/english.pickle')","metadata":{"execution":{"iopub.status.busy":"2022-03-13T00:23:49.978693Z","iopub.execute_input":"2022-03-13T00:23:49.979113Z","iopub.status.idle":"2022-03-13T00:23:50.442141Z","shell.execute_reply.started":"2022-03-13T00:23:49.979066Z","shell.execute_reply":"2022-03-13T00:23:50.44131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !wget \"https://nlp.cs.washington.edu/triviaqa/data/triviaqa-rc.tar.gz\"","metadata":{"execution":{"iopub.status.busy":"2022-03-09T17:49:42.89869Z","iopub.execute_input":"2022-03-09T17:49:42.898949Z","iopub.status.idle":"2022-03-09T17:52:34.541044Z","shell.execute_reply.started":"2022-03-09T17:49:42.898922Z","shell.execute_reply":"2022-03-09T17:52:34.540189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !mkdir ./triviaqa \n# !tar -C ./triviaqa -zxf triviaqa-rc.tar.gz","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-09T17:52:34.543336Z","iopub.execute_input":"2022-03-09T17:52:34.543653Z","iopub.status.idle":"2022-03-09T17:54:19.067453Z","shell.execute_reply.started":"2022-03-09T17:52:34.543614Z","shell.execute_reply":"2022-03-09T17:54:19.066328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# triviaqa_to_squad_format('triviaqa/qa/wikipedia-train.json', 'triviaqa/evidence/wikipedia', 'triviaqa/triviaqa_train.json')","metadata":{"execution":{"iopub.status.busy":"2022-03-09T17:54:19.072406Z","iopub.execute_input":"2022-03-09T17:54:19.072744Z","iopub.status.idle":"2022-03-09T18:13:44.923154Z","shell.execute_reply.started":"2022-03-09T17:54:19.072702Z","shell.execute_reply":"2022-03-09T18:13:44.922358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# triviaqa_to_squad_format('triviaqa/qa/wikipedia-dev.json', 'triviaqa/evidence/wikipedia', 'triviaqa/triviaqa_dev.json')","metadata":{"execution":{"iopub.status.busy":"2022-03-09T18:13:44.928434Z","iopub.execute_input":"2022-03-09T18:13:44.930504Z","iopub.status.idle":"2022-03-09T18:16:16.031449Z","shell.execute_reply.started":"2022-03-09T18:13:44.930464Z","shell.execute_reply":"2022-03-09T18:16:16.030684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we are going to preprocess like SQuAD\n\nI used the datasets library from hugging face.","metadata":{"id":"63U7HoojY7d4"}},{"cell_type":"code","source":"!pip install datasets\nimport datasets","metadata":{"id":"CjtAYmNKY_Ew","outputId":"6753d055-a1b8-46c2-cdc1-5a88b6627db1","execution":{"iopub.status.busy":"2022-03-13T12:09:58.373617Z","iopub.execute_input":"2022-03-13T12:09:58.373839Z","iopub.status.idle":"2022-03-13T12:10:06.588941Z","shell.execute_reply.started":"2022-03-13T12:09:58.373809Z","shell.execute_reply":"2022-03-13T12:10:06.588124Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (1.18.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.26.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.8.1)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.4)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.12.2)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2022.2.0)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.2.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.20.3)\nRequirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets) (21.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets) (3.0.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.3.5)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.62.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (4.11.2)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->datasets) (3.0.6)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.7)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2021.10.8)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.7.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (21.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (5.2.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.6.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2021.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import pyarrow as pa\nimport json\n\ndef jsontodataset(filepath, up_to=None):\n    contexts = []\n    questions = []\n    ids = []\n    answers_list = []\n    count = 0\n    #     print(\"here\")\n    with open(filepath) as f:\n        squad = json.load(f)\n        for article in squad[\"data\"]:\n            title = article.get(\"title\", \"\").strip()\n            for paragraph in article[\"paragraphs\"]:\n                context = paragraph[\"context\"].strip()\n                for qa in paragraph[\"qas\"]:\n                    count+=1\n                    if up_to!=None and count>=up_to: \n                        dataset = datasets.Dataset(pa.Table.from_pydict({'context': contexts, 'question': questions, 'id': ids, 'answers': answers_list}))\n                        return dataset\n                    \n                    question = qa[\"question\"].strip()\n                    id_ = qa[\"id\"]\n\n                    answer_starts = [answer[\"answer_start\"] for answer in qa[\"answers\"]]\n                    answers = [answer[\"text\"].strip() for answer in qa[\"answers\"]]\n\n\n                    contexts.append(context)\n                    questions.append(question)\n                    ids.append(id_)\n                    answers_list.append({\"answer_start\": answer_starts, \"text\": answers,})\n\n    dataset = datasets.Dataset(pa.Table.from_pydict({'context': contexts, 'question': questions, 'id': ids, 'answers': answers_list}))\n    #     print(dataset)\n    return dataset\n","metadata":{"execution":{"iopub.status.busy":"2022-03-13T12:10:06.590777Z","iopub.execute_input":"2022-03-13T12:10:06.591048Z","iopub.status.idle":"2022-03-13T12:10:06.604571Z","shell.execute_reply.started":"2022-03-13T12:10:06.591008Z","shell.execute_reply":"2022-03-13T12:10:06.603868Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# train_dataset = jsontodataset('./triviaqa/triviaqa_train.json')\ntrain_dataset = jsontodataset('../input/trivia-squadformat/triviaqa_train (1).json')","metadata":{"execution":{"iopub.status.busy":"2022-03-13T12:44:54.227615Z","iopub.execute_input":"2022-03-13T12:44:54.228254Z","iopub.status.idle":"2022-03-13T12:45:04.667263Z","shell.execute_reply.started":"2022-03-13T12:44:54.228215Z","shell.execute_reply":"2022-03-13T12:45:04.666471Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# train_dataset = jsontodataset('./triviaqa/triviaqa_dev.json')\nvalidation_dataset = jsontodataset('../input/trivia-squadformat/triviaqa_dev (1).json')","metadata":{"execution":{"iopub.status.busy":"2022-03-13T12:45:04.669275Z","iopub.execute_input":"2022-03-13T12:45:04.669535Z","iopub.status.idle":"2022-03-13T12:45:06.731972Z","shell.execute_reply.started":"2022-03-13T12:45:04.669499Z","shell.execute_reply":"2022-03-13T12:45:06.731200Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"Overview of the feature names of the dataset.","metadata":{}},{"cell_type":"code","source":"train_dataset","metadata":{"execution":{"iopub.status.busy":"2022-03-13T12:45:34.692108Z","iopub.execute_input":"2022-03-13T12:45:34.692694Z","iopub.status.idle":"2022-03-13T12:45:34.698889Z","shell.execute_reply.started":"2022-03-13T12:45:34.692652Z","shell.execute_reply":"2022-03-13T12:45:34.698104Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['context', 'question', 'id', 'answers'],\n    num_rows: 110647\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"Let's print the first example.\n\nWe see that for 'answers' column the dataset contains a dictionary with keys 'text' and 'answer_start', that each contain a list with one element.","metadata":{}},{"cell_type":"code","source":"train_dataset[0]","metadata":{"execution":{"iopub.status.busy":"2022-03-13T12:45:38.066444Z","iopub.execute_input":"2022-03-13T12:45:38.066974Z","iopub.status.idle":"2022-03-13T12:45:38.073346Z","shell.execute_reply.started":"2022-03-13T12:45:38.066913Z","shell.execute_reply":"2022-03-13T12:45:38.072374Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"{'context': \"England is a country that is part of the United Kingdom . It shares land borders with Scotland to the north and Wales to the west . The Irish Sea lies northwest of England and the Celtic Sea lies to the southwest . England is separated from continental Europe by the North Sea to the east and the English Channel to the south . The country covers much of the central and southern part of the island of Great Britain , which lies in the North Atlantic ; and includes over 100 smaller islands such as the Isles of Scilly , and the Isle of Wight . \\n \\n The area now called England was first inhabited by modern humans during the Upper Palaeolithic period , but takes its name from the Angles , one of the Germanic tribes who settled during the 5th and 6th centuries . England became a unified state in the 10th century , and since the Age of Discovery , which began during the 15th century , has had a significant cultural and legal impact on the wider world . The English language , the Anglican Church , and English law – the basis for the common law legal systems of many other countries around the world – developed in England , and the country 's parliamentary system of government has been widely adopted by other nations . The Industrial Revolution began in 18th-century England , transforming its society into the world 's first industrialised nation . \\n \\n England 's terrain mostly comprises low hills and plains , especially in central and southern England . However , there are uplands in the north ( for example , the mountainous Lake District , Pennines , and Yorkshire Dales ) and in the south west ( for example , Dartmoor and the Cotswolds ) . The capital is London , which is the largest metropolitan area in both the United Kingdom and the European Union.According to the European Statistical Agency , London is the largest Larger Urban Zone in the EU , a measure of metropolitan area which comprises a city 's urban core as well as its surrounding commuting zone . London 's municipal population is also the largest in the EU . England 's population of over 53 million comprises 84 % of the population of the United Kingdom , largely concentrated around London , the South East , and conurbations in the Midlands , the North West , the North East , and Yorkshire , which each developed as major industrial regions during the 19th century . [ http : //www.ons.gov.uk/ons/dcp171778_270487.pdf 2011 Census – Population and household estimates for England and Wales , March 2011 ] . Accessed 31 May 2013 . \\n \\n The Kingdom of England—which after 1535 included Wales—ceased being a separate sovereign state on 1 May 1707 , when the Acts of Union put into effect the terms agreed in the Treaty of Union the previous year , resulting in a political union with the Kingdom of Scotland to create the Kingdom of Great Britain . In 1801 , Great Britain was united with the Kingdom of Ireland through another Act of Union to become the United Kingdom of Great Britain and Ireland . In 1922 the Irish Free State seceded from the United Kingdom , leading to the latter being renamed the United Kingdom of Great Britain and Northern Ireland . \\n \\n Toponymy \\n \\n The name `` England '' is derived from the Old English name Englaland , which means `` land of the Angles '' . The Angles were one of the Germanic tribes that settled in Great Britain during the Early Middle Ages . The Angles came from the Angeln peninsula in the Bay of Kiel area of the Baltic Sea . The earliest recorded use of the term , as `` Engla londe '' , is in the late ninth century translation into Old English of Bede 's Ecclesiastical History of the English People . The term was then used in a different sense to the modern one , meaning `` the land inhabited by the English '' , and it included English people in what is now south-east Scotland but was then part of the English kingdom of Northumbria . The Anglo-Saxon Chronicle recorded that the Domesday Book of 1086 covered the whole of England , meaning the English kingdom , but a few years later the Chronicle stated that King Malcolm III went `` out of Scotlande into Lothian in Englaland '' , thus using it in the more ancient sense . According to the Oxford English Dictionary , its modern spelling was first used in 1538 . \\n \\n The earliest attested reference to the Angles occurs in the 1st-century\",\n 'question': 'Where in England was Dame Judi Dench born?',\n 'id': 'tc_3--England.txt',\n 'answers': {'answer_end': 1573, 'answer_start': 1569, 'text': 'York'}}"},"metadata":{}}]},{"cell_type":"markdown","source":"Same features for the validation dataset.","metadata":{}},{"cell_type":"code","source":"validation_dataset","metadata":{"id":"eVVrUwynQGbg","outputId":"2d6c704d-d357-43bd-995e-0c0bf305e377","execution":{"iopub.status.busy":"2022-03-13T15:54:11.658867Z","iopub.execute_input":"2022-03-13T15:54:11.659703Z","iopub.status.idle":"2022-03-13T15:54:11.665204Z","shell.execute_reply.started":"2022-03-13T15:54:11.659655Z","shell.execute_reply":"2022-03-13T15:54:11.664471Z"},"trusted":true},"execution_count":59,"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['context', 'question', 'id', 'answers', 'input_ids', 'attention_mask', 'offset_mapping'],\n    num_rows: 14229\n})"},"metadata":{}}]},{"cell_type":"code","source":"len(validation_dataset['offset_mapping'][4])","metadata":{"execution":{"iopub.status.busy":"2022-03-13T15:56:33.348495Z","iopub.execute_input":"2022-03-13T15:56:33.348768Z","iopub.status.idle":"2022-03-13T15:57:03.695228Z","shell.execute_reply.started":"2022-03-13T15:56:33.348737Z","shell.execute_reply":"2022-03-13T15:57:03.694215Z"},"trusted":true},"execution_count":62,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_1702/996914093.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'offset_mapping'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1931\u001b[0m         \u001b[0;34m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1932\u001b[0m         return self._getitem(\n\u001b[0;32m-> 1933\u001b[0;31m             \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1934\u001b[0m         )\n\u001b[1;32m   1935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_getitem\u001b[0;34m(self, key, decoded, **kwargs)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         \u001b[0mpa_subtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m         formatted_output = format_table(\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0mpa_subtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_all_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m         )\n\u001b[1;32m   1920\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mformatted_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0mpython_formatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPythonFormatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat_columns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"column\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"column\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mformat_column\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mformat_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0mcolumn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_arrow_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mcolumn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_features_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mextract_column\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextract_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pylist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextract_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"If there is no answer, the lists are empty","metadata":{}},{"cell_type":"code","source":"validation_dataset[-10]","metadata":{"id":"JVdlfwpmQK38","outputId":"8cdd0148-f428-4d6e-ef48-a0aa98bc3edc","execution":{"iopub.status.busy":"2022-03-02T18:02:24.774838Z","iopub.execute_input":"2022-03-02T18:02:24.775382Z","iopub.status.idle":"2022-03-02T18:02:24.782185Z","shell.execute_reply.started":"2022-03-02T18:02:24.775342Z","shell.execute_reply":"2022-03-02T18:02:24.781342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = pd.DataFrame(train_dataset)","metadata":{"id":"jiuJexOB9HuG","execution":{"iopub.status.busy":"2022-03-02T01:03:18.400308Z","iopub.execute_input":"2022-03-02T01:03:18.400812Z","iopub.status.idle":"2022-03-02T01:03:34.725387Z","shell.execute_reply.started":"2022-03-02T01:03:18.400774Z","shell.execute_reply":"2022-03-02T01:03:34.724598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df.tail(10)","metadata":{"id":"_wBrvvfK9RAe","outputId":"9f413654-0ec5-4fb6-b1d4-ade24ec0d251","execution":{"iopub.status.busy":"2022-03-02T01:03:34.726638Z","iopub.execute_input":"2022-03-02T01:03:34.726906Z","iopub.status.idle":"2022-03-02T01:03:34.752848Z","shell.execute_reply.started":"2022-03-02T01:03:34.72687Z","shell.execute_reply":"2022-03-02T01:03:34.752066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n","metadata":{"id":"xX13F9Mk-HDw"}},{"cell_type":"markdown","source":"In this exercise I will use distilbert instead of bert because this dataset is larger and distilbert according to documentation is faster. \n> we leverage knowledge distillation during the pre-training phase and show\nthat it is possible to reduce the size of a BERT model by 40%, while retaining 97%\nof its language understanding capabilities and being 60% faster","metadata":{"id":"-_6_2Eg9-RU0"}},{"cell_type":"code","source":"from transformers import DistilBertTokenizerFast, DistilBertForQuestionAnswering\n\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\nmodel = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")","metadata":{"id":"nIUEr1Rq-RU6","outputId":"e85db52c-15b1-4ee5-af17-762dd754df7a","execution":{"iopub.status.busy":"2022-03-13T12:10:30.827377Z","iopub.execute_input":"2022-03-13T12:10:30.827624Z","iopub.status.idle":"2022-03-13T12:10:33.254768Z","shell.execute_reply.started":"2022-03-13T12:10:30.827588Z","shell.execute_reply":"2022-03-13T12:10:33.254008Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Find encodings for BERT with tokenizer. The max length of a sequence (question+anwer with special tokens) is 489, so I use that number for padding.","metadata":{"id":"sQbQUW6v-RU7"}},{"cell_type":"markdown","source":"Preprocessing the dataset","metadata":{"id":"DauZRzlKhglZ"}},{"cell_type":"code","source":"# train_dataset[1]","metadata":{"execution":{"iopub.status.busy":"2022-03-02T18:02:58.870466Z","iopub.execute_input":"2022-03-02T18:02:58.871033Z","iopub.status.idle":"2022-03-02T18:02:58.878123Z","shell.execute_reply.started":"2022-03-02T18:02:58.870994Z","shell.execute_reply":"2022-03-02T18:02:58.877307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the training dataset, I noticed that for each question there is only one answer, so there is no need to keep the values of the answers dictionary in lists. For example: `'answers': {'text': ['singing and dancing'], 'answer_start': [207]}}` can be reformated to `'answers': {'text': 'singing and dancing', 'answer_start': 207}}`. As for questions that are inanswerable (they look like this:`'answers': {'text': [], 'answer_start': []}}` we can just have `'answers': {'text': \"\", 'answer_start': 0}}`.","metadata":{}},{"cell_type":"markdown","source":"Let's find the end character index that we will use to find the end token","metadata":{}},{"cell_type":"code","source":"def find_end(example):\n\n    if (len(example['answers']['text']) != 0):\n        context = example['context']\n        text = example['answers']['text'][0]\n        start_idx = example['answers']['answer_start'][0]\n\n        end_idx = start_idx + len(text)\n        \n        temp = example['answers'] # to change the value\n        temp['answer_end']=end_idx \n        temp['answer_start'] = start_idx # [num]->num\n        temp['text'] = text # ['text']->text\n    \n    else:\n        temp = example['answers']\n        temp['answer_end'] = 0 # []->0\n        temp['answer_start'] = 0 # []->0\n        temp['text'] = \"\" # []->\"\"\n        \n    return example\n\ntrain_dataset = train_dataset.map(find_end)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T12:45:17.180518Z","iopub.execute_input":"2022-03-13T12:45:17.181074Z","iopub.status.idle":"2022-03-13T12:45:33.272481Z","shell.execute_reply.started":"2022-03-13T12:45:17.181032Z","shell.execute_reply":"2022-03-13T12:45:33.271782Z"},"trusted":true},"execution_count":40,"outputs":[{"output_type":"display_data","data":{"text/plain":"0ex [00:00, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f29fcd95f7c7432aafd239d7a98fa20e"}},"metadata":{}}]},{"cell_type":"markdown","source":"Check some examples:","metadata":{}},{"cell_type":"code","source":"train_dataset[1]","metadata":{"execution":{"iopub.status.busy":"2022-03-13T12:46:28.123321Z","iopub.execute_input":"2022-03-13T12:46:28.124163Z","iopub.status.idle":"2022-03-13T12:46:28.131524Z","shell.execute_reply.started":"2022-03-13T12:46:28.124117Z","shell.execute_reply":"2022-03-13T12:46:28.130485Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"{'context': \"Dame Judith Olivia `` Judi '' Dench , ( born 9 December 1934 ) is an English actress and author . Dench made her professional debut in 1957 with the Old Vic Company . Over the following few years she performed in several of Shakespeare 's plays in such roles as Ophelia in Hamlet , Juliet in Romeo and Juliet and Lady Macbeth in Macbeth . Although most of her work during this period was in theatre , she also branched into film work , and won a BAFTA Award as Most Promising Newcomer . She drew strong reviews for her leading role in the musical Cabaret in 1968 . \\n \\n Over the next two decades , Dench established herself as one of the most significant British theatre performers , working for the National Theatre Company and the Royal Shakespeare Company . She achieved success in television during this period , in the series A Fine Romance from 1981 until 1984 , and in 1992 with a starring role in the romantic comedy series As Time Goes By . Her film appearances were infrequent and included supporting roles in major films such as A Room with a View ( 1986 ) supporting Maggie Smith , before she rose to international fame as M in GoldenEye ( 1995 ) , a role she continued to play in James Bond films until Spectre ( 2015 ) . She received her first Oscar nomination for Best Actress for her role as Queen Victoria in Mrs Brown ( 1997 ) and the following year won the Academy Award for Best Supporting Actress for Shakespeare in Love . A seven-time Oscar nominee , she has also received nominations for her roles in Chocolat ( 2000 ) , Iris ( 2001 ) , Mrs Henderson Presents ( 2005 ) , Notes on a Scandal ( 2006 ) , and Philomena ( 2013 ) . \\n \\n Dench has received many award nominations for her acting in theatre , film and television ; her competitive awards include six British Academy Film Awards , four BAFTA TV Awards , seven Olivier Awards , two Screen Actors Guild Awards , two Golden Globes , an Academy Award , and a Tony Award . She has also received the BAFTA Fellowship ( 2001 ) and the Special Olivier Award ( 2004 ) . In June 2011 , she received a fellowship from the British Film Institute ( BFI ) . Dench is also a Fellow of the Royal Society of Arts ( FRSA ) . \\n \\n Early life \\n \\n Dench was born in Heworth , North Riding of Yorkshire . Her mother , Eleanora Olive ( née Jones ) , was born in Dublin . Her father , Reginald Arthur Dench , a doctor , was born in Dorset , and later moved to Dublin , where he was raised . He met Dench 's mother while he was studying medicine at Trinity College , Dublin . \\n \\n Dench attended The Mount School , a Quaker independent secondary school in York , and became a Quaker . Her brothers , one of whom was actor Jeffery Dench , were born in Tyldesley , Lancashire . Her niece , Emma Dench , is a Roman historian and professor previously at Birkbeck , University of London , and currently at Harvard University . \\n \\n Career \\n \\n In Britain , Dench has developed a reputation as one of the greatest actresses of the post-war period , primarily through her work in theatre , which has been her forte throughout her career . She has more than once been named number one in polls for Britain 's best actor . \\n \\n Early years \\n \\n Through her parents , Dench had regular contact with the theatre . Her father , a physician , was also the GP for the York theatre , and her mother was its wardrobe mistress . Actors often stayed in the Dench household . During these years , Judi Dench was involved on a non-professional basis in the first three productions of the modern revival of the York Mystery Plays in the 1950s . In 1957 , in one of the last productions in which she appeared during this period , she played the role of the Virgin Mary , performed on a fixed stage in the Museum Gardens . Though she initially trained as a set designer , she became interested in drama school as her brother Jeff attended the Central School of Speech and Drama . She applied and was accepted , where she was a classmate of Vanessa Redgrave , graduating with a first class degree in drama and four acting prizes , one being the Gold Medal as Outstanding\",\n 'question': 'Where in England was Dame Judi Dench born?',\n 'id': 'tc_3--Judi_Dench.txt',\n 'answers': {'answer_end': 2252, 'answer_start': 2248, 'text': 'York'}}"},"metadata":{}}]},{"cell_type":"markdown","source":"Example with no answer","metadata":{}},{"cell_type":"markdown","source":"Tokenize train dataset and find end and start tokens. The sequence lenght will be 512, the maximum one for bert.","metadata":{}},{"cell_type":"code","source":"def encode(examples):\n    tokenized = tokenizer(examples['context'], examples['question'], truncation=True, padding=True)\n    start_token_list = []\n    end_token_list = []\n    answers = examples['answers']\n    for i in range(len(answers)):\n        if (answers[i]['text'] != ''):\n            start_token = tokenized.char_to_token(i, answers[i]['answer_start'])\n            end_token = tokenized.char_to_token(i, answers[i]['answer_end'] - 1)\n            \n            # if start token is None, the answer passage has been truncated\n            if start_token is None:\n                start_token = tokenizer.model_max_length\n            if end_token is None:\n                end_token = tokenizer.model_max_length\n        else:\n            start_token = 0\n            end_token = 0\n            \n        start_token_list.append(start_token)\n        end_token_list.append(end_token)\n            \n\n    return {\"start_position\": start_token_list, \"end_position\": end_token_list, \"input_ids\": tokenized['input_ids'], \"attention_mask\": tokenized['attention_mask']}\n\ntrain_dataset = train_dataset.map(encode, batched=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T12:46:30.395644Z","iopub.execute_input":"2022-03-13T12:46:30.396216Z","iopub.status.idle":"2022-03-13T12:50:58.872674Z","shell.execute_reply.started":"2022-03-13T12:46:30.396174Z","shell.execute_reply":"2022-03-13T12:50:58.871891Z"},"trusted":true},"execution_count":44,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/111 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a782044811f64e50b5d8c1e59a184f19"}},"metadata":{}}]},{"cell_type":"markdown","source":"The train dataset is now updated with the columns 'start_position', 'end_position', 'input_ids' and 'attention_mask'.","metadata":{}},{"cell_type":"code","source":"train_dataset","metadata":{"execution":{"iopub.status.busy":"2022-03-13T12:50:58.883556Z","iopub.execute_input":"2022-03-13T12:50:58.883974Z","iopub.status.idle":"2022-03-13T12:50:58.891461Z","shell.execute_reply.started":"2022-03-13T12:50:58.883915Z","shell.execute_reply":"2022-03-13T12:50:58.890695Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['context', 'question', 'id', 'answers'],\n    num_rows: 110647\n})"},"metadata":{}}]},{"cell_type":"code","source":"batch_size = 8\ntrain_data = TensorDataset(torch.tensor(train_dataset['input_ids'], dtype=torch.int64), \n                           torch.tensor(train_dataset['attention_mask'], dtype=torch.float), \n                           torch.tensor(train_dataset['start_position'], dtype=torch.int64), \n                           torch.tensor(train_dataset['start_position'], dtype=torch.int64))\n\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)","metadata":{"id":"0e4Pv5Kshgla","execution":{"iopub.status.busy":"2022-03-13T12:52:59.943462Z","iopub.execute_input":"2022-03-13T12:52:59.944034Z","iopub.status.idle":"2022-03-13T12:54:04.799220Z","shell.execute_reply.started":"2022-03-13T12:52:59.943992Z","shell.execute_reply":"2022-03-13T12:54:04.798410Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"Validation dataset does not need that much preprocessing. I pass to the dataloader only the input_ids, token_type_ids and attention masks, that will be passed to bert model in batches. I use a Sequential sampler to keep the indexing same as the validation dataset. We will need the offsets mapping to construct the sentence from the predicted start and end tokens and compare it with the actual answers.","metadata":{}},{"cell_type":"code","source":"validation_dataset = validation_dataset.map(lambda examples: tokenizer(examples['context'], examples['question'], truncation=True, padding=True, return_offsets_mapping=True), batched=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T15:44:25.852829Z","iopub.execute_input":"2022-03-13T15:44:25.853731Z","iopub.status.idle":"2022-03-13T15:47:15.538242Z","shell.execute_reply.started":"2022-03-13T15:44:25.853686Z","shell.execute_reply":"2022-03-13T15:47:15.537460Z"},"trusted":true},"execution_count":55,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/15 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68cc5b11131a470eb218f10e9366275a"}},"metadata":{}}]},{"cell_type":"code","source":"batch_size = 8\nval_data = TensorDataset(torch.tensor(validation_dataset['input_ids'], dtype=torch.int64), \n                        torch.tensor(validation_dataset['attention_mask'], dtype=torch.float))\nval_sampler = SequentialSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T15:47:15.539999Z","iopub.execute_input":"2022-03-13T15:47:15.540270Z","iopub.status.idle":"2022-03-13T15:47:23.443014Z","shell.execute_reply.started":"2022-03-13T15:47:15.540233Z","shell.execute_reply":"2022-03-13T15:47:23.442257Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## Initialize & train functions","metadata":{"id":"MReVWUdjhgla"}},{"cell_type":"markdown","source":"The training for each epoch took aprox. 2 hours so I couldn't try many epochs and do many runs when using the whole dataset.","metadata":{}},{"cell_type":"markdown","source":"For optimizer, I used AdamW (Adam with weight decay) which is the one that was used in BERT during pre-training. ","metadata":{}},{"cell_type":"code","source":"epochs = 3\nmodel.to(device)\noptimizer = optim.AdamW(model\n                        .parameters(), lr=5e-5)","metadata":{"id":"4ezESoUuhgla","execution":{"iopub.status.busy":"2022-03-13T12:54:36.267133Z","iopub.execute_input":"2022-03-13T12:54:36.267614Z","iopub.status.idle":"2022-03-13T12:54:38.583743Z","shell.execute_reply.started":"2022-03-13T12:54:36.267576Z","shell.execute_reply":"2022-03-13T12:54:38.582979Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n# model.load_state_dict(torch.load(\"./weights_\" + str(0) + \".pth\"))\nfor epoch in range(epochs):\n    epoch_loss = []\n    validation_loss = []\n    \n    total_loss = 0\n    model.train()\n\n    count=-1\n    progress_bar = tqdm(train_dataloader, leave=True, position=0)\n    progress_bar.set_description(f\"Epoch {epoch+1}\")\n    for batch in progress_bar:\n        count+=1\n        input_ids, mask, start, end  = tuple(t.to(device) for t in batch)\n\n        model.zero_grad()\n        loss, start_logits, end_logits = model(input_ids = input_ids, \n                                                attention_mask = mask, \n                                                start_positions = start, \n                                                end_positions = end,\n                                                return_dict = False)           \n\n        total_loss += loss.item()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        \n        if (count % 20 == 0 and count != 0):\n            avg = total_loss/count\n            progress_bar.set_postfix(Loss=avg)\n            \n    torch.save(model.state_dict(), \"./trivia\" + str(epoch) + \".h5\") # save for later use\n    avg_train_loss = total_loss / len(train_dataloader)\n    epoch_loss.append(avg_train_loss)\n    print(f\"Epoch {epoch} Loss: {avg_train_loss}\\n\")","metadata":{"id":"b0Cp_bo4QV_S","outputId":"c1b55f94-6519-47f1-df5a-18e2ed33b7c4","execution":{"iopub.status.busy":"2022-03-13T12:54:38.585497Z","iopub.execute_input":"2022-03-13T12:54:38.585763Z","iopub.status.idle":"2022-03-13T15:39:29.425262Z","shell.execute_reply.started":"2022-03-13T12:54:38.585725Z","shell.execute_reply":"2022-03-13T15:39:29.424442Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 13831/13831 [54:54<00:00,  4.20it/s, Loss=2.1] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 0 Loss: 2.1038754528256267\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 13831/13831 [55:04<00:00,  4.19it/s, Loss=1.37]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Loss: 1.3681465321896578\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 13831/13831 [54:49<00:00,  4.20it/s, Loss=1.03]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 Loss: 1.0256071441891226\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# EVALUATION\nevaluate trivia qa on trivia qa\n\n","metadata":{"id":"do-qX2pEgCMU"}},{"cell_type":"code","source":"from tqdm import tqdm\nmodel.load_state_dict(torch.load(\"./trivia2.h5\"))\nmodel.to(device)\n\nthreshold = 1.0\nepoch_i = 0\ncorrect = 0 \npred_dict = {}\nna_prob_dict = {}\n\nmodel.eval()\ncorrect = 0\nbatch_val_losses = []\nrow = 0\nfor test_batch in tqdm(val_dataloader):\n    input_ids, masks = tuple(t.to(device) for t in test_batch)\n\n    with torch.no_grad():\n        # prediction logits\n        start_logits, end_logits = model(input_ids=input_ids,\n                                        attention_mask=masks,\n                                        return_dict=False)\n\n    # to cpu\n    start_logits = start_logits.detach().cpu()\n    end_logits = end_logits.detach().cpu()\n\n    # for every sequence in batch \n    for bidx in range(len(start_logits)):\n        # apply softmax to logits to get scores\n        start_scores = np.array(F.softmax(start_logits[bidx], dim = 0))\n        end_scores = np.array(F.softmax(end_logits[bidx], dim = 0))\n\n        # find max for start<=end\n        size = len(start_scores)\n        scores = np.zeros((size, size))\n        for j in range(size):\n            for i in range(j+1): # include j\n                scores[i,j] = start_scores[i] + end_scores[j]\n\n        # find best i and j\n        start_pred, end_pred = unravel_index(scores.argmax(), scores.shape)\n        answer_pred = \"\"\n        if (scores[start_pred, end_pred] > scores[0,0]+threshold):\n\n            offsets = validation_dataset[row]['offset_mapping']\n            pred_char_start = offsets[start_pred][0]\n\n            if end_pred < len(offsets):\n                pred_char_end = offsets[end_pred][1]\n                answer_pred = validation_dataset[row]['context'][pred_char_start:pred_char_end]\n            else:\n                answer_pred = validation_dataset[row]['context'][pred_char_start:]\n                \n            if answer_pred in validation_dataset[row]['answers']['text']:\n                correct += 1\n\n        else:\n            if (len(validation_dataset[row]['answers']['text']) ==0):\n                correct += 1        \n\n        pred_dict[validation_dataset[row]['id']] = answer_pred\n        na_prob_dict[validation_dataset[row]['id']] = scores[0,0]\n\n        row+=1\n\n\naccuracy = correct/validation_dataset.num_rows\nprint(\"accuracy is: \", accuracy)","metadata":{"id":"PRFIYVtdQ07R","outputId":"bcac5809-99fe-4cae-dbae-ea6cce76a1a7","execution":{"iopub.status.busy":"2022-03-13T15:57:19.508014Z","iopub.execute_input":"2022-03-13T15:57:19.508519Z","iopub.status.idle":"2022-03-13T16:29:46.700632Z","shell.execute_reply.started":"2022-03-13T15:57:19.508485Z","shell.execute_reply":"2022-03-13T16:29:46.699754Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stderr","text":"100%|██████████| 1779/1779 [32:26<00:00,  1.09s/it]","output_type":"stream"},{"name":"stdout","text":"accuracy is:  0.3723381825848619\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"import json \nwith open(\"pred.json\", \"w\") as outfile:\n    json.dump(pred_dict, outfile)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T16:42:19.716740Z","iopub.execute_input":"2022-03-13T16:42:19.717029Z","iopub.status.idle":"2022-03-13T16:42:19.750823Z","shell.execute_reply.started":"2022-03-13T16:42:19.716996Z","shell.execute_reply":"2022-03-13T16:42:19.750070Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"import json \nwith open(\"na_prob.json\", \"w\") as outfile:\n    json.dump(na_prob_dict, outfile)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T16:42:21.009708Z","iopub.execute_input":"2022-03-13T16:42:21.010140Z","iopub.status.idle":"2022-03-13T16:42:21.065932Z","shell.execute_reply.started":"2022-03-13T16:42:21.010101Z","shell.execute_reply":"2022-03-13T16:42:21.065141Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"for i in range(5):\n    print(f\"Question: {validation_dataset[i]['question']}\")\n    print(f\"Predicted answer: {pred_dict[validation_dataset[i]['id']]}\")\n    print(f\"Answers: {validation_dataset[i]['answers']['text']}\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-03-13T11:27:19.683291Z","iopub.execute_input":"2022-03-13T11:27:19.683557Z","iopub.status.idle":"2022-03-13T11:27:19.696016Z","shell.execute_reply.started":"2022-03-13T11:27:19.683527Z","shell.execute_reply":"2022-03-13T11:27:19.695219Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Question: Which Lloyd Webber musical premiered in the US on 10th December 1993?\nPredicted answer: Cats\nAnswers: []\n\nQuestion: Who was the next British Prime Minister after Arthur Balfour?\nPredicted answer: \nAnswers: []\n\nQuestion: Who was the next British Prime Minister after Arthur Balfour?\nPredicted answer: \nAnswers: []\n\nQuestion: Who had a 70s No 1 hit with Kiss You All Over?\nPredicted answer: Exile\nAnswers: ['Exile']\n\nQuestion: What claimed the life of singer Kathleen Ferrier?\nPredicted answer: \nAnswers: ['Cancer']\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Download the official evaluation script","metadata":{}},{"cell_type":"code","source":"!wget https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/ -O evaluation.py","metadata":{"execution":{"iopub.status.busy":"2022-03-13T16:42:25.668985Z","iopub.execute_input":"2022-03-13T16:42:25.669697Z","iopub.status.idle":"2022-03-13T16:42:27.012666Z","shell.execute_reply.started":"2022-03-13T16:42:25.669649Z","shell.execute_reply":"2022-03-13T16:42:27.011812Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n--2022-03-13 16:42:26--  https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\nResolving worksheets.codalab.org (worksheets.codalab.org)... 13.68.212.115\nConnecting to worksheets.codalab.org (worksheets.codalab.org)|13.68.212.115|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nSyntax error in Set-Cookie: codalab_session=\"\"; expires=Thu, 01 Jan 1970 00:00:00 GMT; Max-Age=-1; Path=/ at position 70.\nLength: unspecified [text/x-python]\nSaving to: ‘evaluation.py’\n\nevaluation.py           [ <=>                ]  10.30K  --.-KB/s    in 0s      \n\n2022-03-13 16:42:26 (121 MB/s) - ‘evaluation.py’ saved [10547]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!python evaluation.py \"../input/trivia-squadformat/triviaqa_dev (1).json\" pred.json --na-prob-file na_prob.json","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-13T16:42:30.279476Z","iopub.execute_input":"2022-03-13T16:42:30.279786Z","iopub.status.idle":"2022-03-13T16:42:34.898509Z","shell.execute_reply.started":"2022-03-13T16:42:30.279753Z","shell.execute_reply":"2022-03-13T16:42:34.897589Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n{\n  \"exact\": 37.739827113641155,\n  \"f1\": 44.6628950265895,\n  \"total\": 14229,\n  \"HasAns_exact\": 13.005486689697216,\n  \"HasAns_f1\": 23.014461830251516,\n  \"HasAns_total\": 9842,\n  \"NoAns_exact\": 93.22999772053795,\n  \"NoAns_f1\": 93.22999772053795,\n  \"NoAns_total\": 4387,\n  \"best_exact\": 37.753882915173236,\n  \"best_exact_thresh\": 0.37428009510040283,\n  \"best_f1\": 44.64415395788189,\n  \"best_f1_thresh\": 0.48292508721351624,\n  \"pr_exact_ap\": 3.52097137469337,\n  \"pr_f1_ap\": 9.97684824537978,\n  \"pr_oracle_ap\": 93.16980559099022\n}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Evaluate trivia QA on SQuAD","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nvalidation_squad = load_dataset('squad_v2', split='validation')","metadata":{"execution":{"iopub.status.busy":"2022-03-13T16:42:55.320313Z","iopub.execute_input":"2022-03-13T16:42:55.320615Z","iopub.status.idle":"2022-03-13T16:43:09.192033Z","shell.execute_reply.started":"2022-03-13T16:42:55.320582Z","shell.execute_reply":"2022-03-13T16:43:09.191271Z"},"trusted":true},"execution_count":70,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.87k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71fe382d20824ff99a063690fa5d8a63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.02k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"663e2111e03246c399b97d11d90a2d30"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset squad_v2/squad_v2 (download: 44.34 MiB, generated: 122.41 MiB, post-processed: Unknown size, total: 166.75 MiB) to /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f051d51b39174311b4f1ba475d50be0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/9.55M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14864e327edb44c0adabcad6fba36f2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/801k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bec80c4b3974ce28052b2a236cbdf7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfafea8d19b146bd91dae8b3595dab11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset squad_v2 downloaded and prepared to /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d. Subsequent calls will reuse this data.\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenized_validation_squad = tokenizer(validation_squad['context'], validation_squad['question'], truncation=True, padding=True, return_offsets_mapping=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T16:43:20.081494Z","iopub.execute_input":"2022-03-13T16:43:20.082108Z","iopub.status.idle":"2022-03-13T16:43:27.682624Z","shell.execute_reply.started":"2022-03-13T16:43:20.082060Z","shell.execute_reply":"2022-03-13T16:43:27.681872Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"batch_size = 8\nval_data = TensorDataset(torch.tensor(tokenized_validation_squad['input_ids'], dtype=torch.int64),\n                        torch.tensor(tokenized_validation_squad['attention_mask'], dtype=torch.float))\nval_sampler = SequentialSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T16:43:51.498580Z","iopub.execute_input":"2022-03-13T16:43:51.499069Z","iopub.status.idle":"2022-03-13T16:43:51.945294Z","shell.execute_reply.started":"2022-03-13T16:43:51.499030Z","shell.execute_reply":"2022-03-13T16:43:51.944395Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"../input/question3-train/trivia2.h5\"))\n\nthreshold = 1.0\nepoch_i = 0\ncorrect = 0 \npred_dict = {}\nna_prob_dict = {}\n\nmodel.eval()\ncorrect = 0\nbatch_val_losses = []\nrow = 0\nfor test_batch in tqdm(val_dataloader):\n    input_ids, masks = tuple(t.to(device) for t in test_batch)\n\n    with torch.no_grad():\n        # prediction logits\n        start_logits, end_logits = model(input_ids=input_ids,\n                                        attention_mask=masks,\n                                        return_dict=False)\n\n    # to cpu\n    start_logits = start_logits.detach().cpu()\n    end_logits = end_logits.detach().cpu()\n\n    # for every sequence in batch \n    for bidx in range(len(start_logits)):\n        # apply softmax to logits to get scores\n        start_scores = np.array(F.softmax(start_logits[bidx], dim = 0))\n        end_scores = np.array(F.softmax(end_logits[bidx], dim = 0))\n\n        # find max for start<=end\n        size = len(start_scores)\n        scores = np.zeros((size, size))\n        for j in range(size):\n            for i in range(j+1): # include j\n                scores[i,j] = start_scores[i] + end_scores[j]\n\n        # find best i and j\n        start_pred, end_pred = unravel_index(scores.argmax(), scores.shape)\n        answer_pred = \"\"\n        if (scores[start_pred, end_pred] > scores[0,0]+threshold):\n\n            offsets = tokenized_validation_squad.offset_mapping[row]\n            pred_char_start = offsets[start_pred][0]\n\n            if end_pred < len(offsets):\n                pred_char_end = offsets[end_pred][1]\n                answer_pred = validation_squad[row]['context'][pred_char_start:pred_char_end]\n            else:\n                answer_pred = validation_squad[row]['context'][pred_char_start:]\n                \n            if answer_pred in validation_squad[row]['answers']['text']:\n                correct += 1\n\n        else:\n            if (len(validation_squad[row]['answers']['text']) ==0):\n                correct += 1        \n\n        pred_dict[validation_squad[row]['id']] = answer_pred\n        na_prob_dict[validation_squad[row]['id']] = scores[0,0]\n\n        row+=1\n\n\naccuracy = correct/validation_squad.num_rows\nprint(\"accuracy is: \", accuracy)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T16:43:55.944051Z","iopub.execute_input":"2022-03-13T16:43:55.944468Z","iopub.status.idle":"2022-03-13T17:09:07.787733Z","shell.execute_reply.started":"2022-03-13T16:43:55.944433Z","shell.execute_reply":"2022-03-13T17:09:07.786968Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stderr","text":"100%|██████████| 1485/1485 [25:11<00:00,  1.02s/it]","output_type":"stream"},{"name":"stdout","text":"accuracy is:  0.3098627137202055\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"import json \nwith open(\"pred2.json\", \"w\") as outfile:\n    json.dump(pred_dict, outfile)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T17:11:48.479863Z","iopub.execute_input":"2022-03-13T17:11:48.480594Z","iopub.status.idle":"2022-03-13T17:11:48.511552Z","shell.execute_reply.started":"2022-03-13T17:11:48.480554Z","shell.execute_reply":"2022-03-13T17:11:48.510812Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"import json \nwith open(\"na_prob2.json\", \"w\") as outfile:\n    json.dump(na_prob_dict, outfile)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T17:11:49.671476Z","iopub.execute_input":"2022-03-13T17:11:49.671742Z","iopub.status.idle":"2022-03-13T17:11:49.722673Z","shell.execute_reply.started":"2022-03-13T17:11:49.671712Z","shell.execute_reply":"2022-03-13T17:11:49.721884Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"markdown","source":"Get SQuAD dev.json","metadata":{}},{"cell_type":"code","source":"!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json -O squad_dev.json","metadata":{"execution":{"iopub.status.busy":"2022-03-13T17:11:51.247180Z","iopub.execute_input":"2022-03-13T17:11:51.247463Z","iopub.status.idle":"2022-03-13T17:11:52.311260Z","shell.execute_reply.started":"2022-03-13T17:11:51.247432Z","shell.execute_reply":"2022-03-13T17:11:52.310347Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n--2022-03-13 17:11:51--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\nResolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.111.153, ...\nConnecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 4370528 (4.2M) [application/json]\nSaving to: ‘squad_dev.json’\n\nsquad_dev.json      100%[===================>]   4.17M  --.-KB/s    in 0.1s    \n\n2022-03-13 17:11:52 (32.2 MB/s) - ‘squad_dev.json’ saved [4370528/4370528]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!python evaluation.py squad_dev.json pred2.json --na-prob-file na_prob2.json","metadata":{"execution":{"iopub.status.busy":"2022-03-13T17:12:04.500738Z","iopub.execute_input":"2022-03-13T17:12:04.501086Z","iopub.status.idle":"2022-03-13T17:12:09.418605Z","shell.execute_reply.started":"2022-03-13T17:12:04.501047Z","shell.execute_reply":"2022-03-13T17:12:09.417725Z"},"trusted":true},"execution_count":78,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n{\n  \"exact\": 31.087341025856986,\n  \"f1\": 33.13962656169863,\n  \"total\": 11873,\n  \"HasAns_exact\": 6.275303643724697,\n  \"HasAns_f1\": 10.385760149637195,\n  \"HasAns_total\": 5928,\n  \"NoAns_exact\": 55.82842724978974,\n  \"NoAns_f1\": 55.82842724978974,\n  \"NoAns_total\": 5945,\n  \"best_exact\": 50.07159100480081,\n  \"best_exact_thresh\": 0.0,\n  \"best_f1\": 50.077205985569506,\n  \"best_f1_thresh\": 3.3368636650266126e-05,\n  \"pr_exact_ap\": 0.5230665528554378,\n  \"pr_f1_ap\": 1.4343411559985633,\n  \"pr_oracle_ap\": 51.01587119837339\n}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Evaluate squad on squad (distilbert)","metadata":{}},{"cell_type":"markdown","source":"First, fine tune squad with distilbert ","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\ntrain_squad = load_dataset('squad_v2', split='train')","metadata":{"execution":{"iopub.status.busy":"2022-03-13T17:49:02.033173Z","iopub.execute_input":"2022-03-13T17:49:02.033432Z","iopub.status.idle":"2022-03-13T17:49:02.329572Z","shell.execute_reply.started":"2022-03-13T17:49:02.033403Z","shell.execute_reply":"2022-03-13T17:49:02.328803Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"train_squad = train_squad.map(find_end)\n\ndef encode(examples):\n    tokenized = tokenizer(examples['context'], examples['question'], truncation=True, padding='max_length', max_length=512)\n    answers = examples['answers']\n    start_token_list = []\n    end_token_list = []\n    for i in range(len(answers)):\n        if (answers[i]['text'] != ''):\n            start_token = tokenized.char_to_token(i, answers[i]['answer_start'])\n            end_token = tokenized.char_to_token(i, answers[i]['answer_end'] - 1)\n            \n            # if start token is None, the answer passage has been truncated\n            if start_token is None:\n                start_token = tokenizer.model_max_length\n            if end_token is None:\n                end_token = tokenizer.model_max_length\n        else:\n            start_token = 0\n            end_token = 0\n            \n        start_token_list.append(start_token)\n        end_token_list.append(end_token)\n            \n\n    return {\"start_position\": start_token_list, \"end_position\": end_token_list, \"input_ids\": tokenized['input_ids'], \"attention_mask\": tokenized['attention_mask']}\n\ntrain_squad = train_squad.map(encode, batched=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T17:49:04.304255Z","iopub.execute_input":"2022-03-13T17:49:04.304707Z","iopub.status.idle":"2022-03-13T17:50:39.246449Z","shell.execute_reply.started":"2022-03-13T17:49:04.304672Z","shell.execute_reply":"2022-03-13T17:50:39.245701Z"},"trusted":true},"execution_count":108,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/131 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e37d2138a3474401a46c246123389fb4"}},"metadata":{}}]},{"cell_type":"code","source":"len(train_squad['input_ids'][1])","metadata":{"execution":{"iopub.status.busy":"2022-03-13T17:50:39.248795Z","iopub.execute_input":"2022-03-13T17:50:39.249092Z","iopub.status.idle":"2022-03-13T17:51:15.596556Z","shell.execute_reply.started":"2022-03-13T17:50:39.249053Z","shell.execute_reply":"2022-03-13T17:51:15.595832Z"},"trusted":true},"execution_count":109,"outputs":[{"execution_count":109,"output_type":"execute_result","data":{"text/plain":"512"},"metadata":{}}]},{"cell_type":"code","source":"batch_size = 8\ntrain_data = TensorDataset(torch.tensor(train_squad['input_ids'], dtype=torch.int64), \n                           torch.tensor(train_squad['attention_mask'], dtype=torch.float), \n                           torch.tensor(train_squad['start_position'], dtype=torch.int64), \n                           torch.tensor(train_squad['start_position'], dtype=torch.int64))\n\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T17:51:15.597790Z","iopub.execute_input":"2022-03-13T17:51:15.598076Z","iopub.status.idle":"2022-03-13T17:52:32.625964Z","shell.execute_reply.started":"2022-03-13T17:51:15.598038Z","shell.execute_reply":"2022-03-13T17:52:32.625159Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\nepochs = 3\nmodel.to(device)\noptimizer = optim.AdamW(model.parameters(), lr=5e-5)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T17:58:00.992784Z","iopub.execute_input":"2022-03-13T17:58:00.993355Z","iopub.status.idle":"2022-03-13T17:58:02.190937Z","shell.execute_reply.started":"2022-03-13T17:58:00.993316Z","shell.execute_reply":"2022-03-13T17:58:02.190176Z"},"trusted":true},"execution_count":113,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from tqdm import tqdm\n# model.load_state_dict(torch.load(\"./weights_\" + str(0) + \".pth\"))\nfor epoch in range(epochs):\n    epoch_loss = []\n    validation_loss = []\n    \n    total_loss = 0\n    model.train()\n\n    count=-1\n    progress_bar = tqdm(train_dataloader, leave=True, position=0)\n    progress_bar.set_description(f\"Epoch {epoch+1}\")\n    for batch in progress_bar:\n        count+=1\n        input_ids, mask, start, end  = tuple(t.to(device) for t in batch)\n\n        model.zero_grad()\n        loss, start_logits, end_logits = model(input_ids = input_ids, \n                                                attention_mask = mask, \n                                                start_positions = start, \n                                                end_positions = end,\n                                                return_dict = False)           \n\n        total_loss += loss.item()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        \n        if (count % 20 == 0 and count != 0):\n            avg = total_loss/count\n            progress_bar.set_postfix(Loss=avg)\n            \n    torch.save(model.state_dict(), \"./squad\" + str(epoch) + \".h5\") # save for later use\n    avg_train_loss = total_loss / len(train_dataloader)\n    epoch_loss.append(avg_train_loss)\n    print(f\"Epoch {epoch} Loss: {avg_train_loss}\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-03-13T17:58:03.585258Z","iopub.execute_input":"2022-03-13T17:58:03.586195Z","iopub.status.idle":"2022-03-13T17:58:29.972205Z","shell.execute_reply.started":"2022-03-13T17:58:03.586142Z","shell.execute_reply":"2022-03-13T17:58:29.970997Z"},"trusted":true},"execution_count":114,"outputs":[{"name":"stderr","text":"Epoch 1:   1%|          | 109/16290 [00:26<1:05:12,  4.14it/s, Loss=3.73]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_1702/3791534224.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"tokenized_validation = tokenizer(validation_squad['context'], validation_squad['question'], truncation=True, padding=True, return_offsets_mapping=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 16\nval_data = TensorDataset(torch.tensor(tokenized_validation['input_ids'], dtype=torch.int64), \n                        torch.tensor(tokenized_validation['attention_mask'], dtype=torch.float))\nval_sampler = SequentialSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"./squad2.h5\"))\n\nthreshold = 1.0\nepoch_i = 0\ncorrect = 0 \npred_dict = {}\nna_prob_dict = {}\n\nmodel.eval()\ncorrect = 0\nbatch_val_losses = []\nrow = 0\nfor test_batch in tqdm(val_dataloader):\n    input_ids, masks = tuple(t.to(device) for t in test_batch)\n\n    with torch.no_grad():\n        # prediction logits\n        start_logits, end_logits = model(input_ids=input_ids,\n                                        attention_mask=masks,\n                                        return_dict=False)\n\n    # to cpu\n    start_logits = start_logits.detach().cpu()\n    end_logits = end_logits.detach().cpu()\n\n    # for every sequence in batch \n    for bidx in range(len(start_logits)):\n        # apply softmax to logits to get scores\n        start_scores = np.array(F.softmax(start_logits[bidx], dim = 0))\n        end_scores = np.array(F.softmax(end_logits[bidx], dim = 0))\n\n        # find max for start<=end\n        size = len(start_scores)\n        scores = np.zeros((size, size))\n        for j in range(size):\n            for i in range(j+1): # include j\n                scores[i,j] = start_scores[i] + end_scores[j]\n\n        # find best i and j\n        start_pred, end_pred = unravel_index(scores.argmax(), scores.shape)\n        answer_pred = \"\"\n        if (scores[start_pred, end_pred] > scores[0,0]+threshold):\n\n            offsets = tokenized_validation_squad.offset_mapping[row]\n            pred_char_start = offsets[start_pred][0]\n\n            if end_pred < len(offsets):\n                pred_char_end = offsets[end_pred][1]\n                answer_pred = validation_squad[row]['context'][pred_char_start:pred_char_end]\n            else:\n                answer_pred = validation_squad[row]['context'][pred_char_start:]\n                \n            if answer_pred in validation_squad[row]['answers']['text']:\n                correct += 1\n\n        else:\n            if (len(validation_squad[row]['answers']['text']) ==0):\n                correct += 1        \n\n        pred_dict[validation_squad[row]['id']] = answer_pred\n        na_prob_dict[validation_squad[row]['id']] = scores[0,0]\n\n        row+=1\n\n\naccuracy = correct/validation_squad.num_rows\nprint(\"accuracy is: \", accuracy)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json \nwith open(\"pred3.json\", \"w\") as outfile:\n    json.dump(pred_dict, outfile)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json \nwith open(\"na_prob3.json\", \"w\") as outfile:\n    json.dump(na_prob_dict, outfile)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python evaluation.py squad_dev.json pred3.json --na-prob-file na_prob3.json","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate squad on trivia QA","metadata":{}},{"cell_type":"code","source":"validation_dataset = validation_dataset.map(lambda examples: tokenizer(examples['context'], examples['question'], truncation=True, padding=True, return_offsets_mapping=True), batched=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T17:59:04.741819Z","iopub.execute_input":"2022-03-13T17:59:04.742385Z","iopub.status.idle":"2022-03-13T18:02:48.330027Z","shell.execute_reply.started":"2022-03-13T17:59:04.742348Z","shell.execute_reply":"2022-03-13T18:02:48.329242Z"},"trusted":true},"execution_count":116,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/15 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69b7a176a2ec443aaa5e609ad194f61b"}},"metadata":{}}]},{"cell_type":"code","source":"batch_size = 8\nval_data = TensorDataset(torch.tensor(validation_dataset['input_ids'], dtype=torch.int64), \n                        torch.tensor(validation_dataset['attention_mask'], dtype=torch.float))\nval_sampler = SequentialSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T18:02:48.331992Z","iopub.execute_input":"2022-03-13T18:02:48.332333Z","iopub.status.idle":"2022-03-13T18:02:56.552877Z","shell.execute_reply.started":"2022-03-13T18:02:48.332293Z","shell.execute_reply":"2022-03-13T18:02:56.552081Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nmodel = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\nmodel.load_state_dict(torch.load(\"../input/bert-weights/weights_2.h5\"))\nmodel.to(device)\n\nthreshold = 1.0\nepoch_i = 0\ncorrect = 0 \npred_dict = {}\nna_prob_dict = {}\n\nmodel.eval()\ncorrect = 0\nbatch_val_losses = []\nrow = 0\nfor test_batch in tqdm(val_dataloader):\n    input_ids, masks = tuple(t.to(device) for t in test_batch)\n\n    with torch.no_grad():\n        # prediction logits\n        start_logits, end_logits = model(input_ids=input_ids,\n                                        attention_mask=masks,\n                                        return_dict=False)\n\n    # to cpu\n    start_logits = start_logits.detach().cpu()\n    end_logits = end_logits.detach().cpu()\n\n    # for every sequence in batch \n    for bidx in range(len(start_logits)):\n        # apply softmax to logits to get scores\n        start_scores = np.array(F.softmax(start_logits[bidx], dim = 0))\n        end_scores = np.array(F.softmax(end_logits[bidx], dim = 0))\n\n        # find max for start<=end\n        size = len(start_scores)\n        scores = np.zeros((size, size))\n        for j in range(size):\n            for i in range(j+1): # include j\n                scores[i,j] = start_scores[i] + end_scores[j]\n\n        # find best i and j\n        start_pred, end_pred = unravel_index(scores.argmax(), scores.shape)\n        answer_pred = \"\"\n        if (scores[start_pred, end_pred] > scores[0,0]+threshold):\n\n            offsets = validation_dataset[row]['offset_mapping']\n            pred_char_start = offsets[start_pred][0]\n\n            if end_pred < len(offsets):\n                pred_char_end = offsets[end_pred][1]\n                answer_pred = validation_dataset[row]['context'][pred_char_start:pred_char_end]\n            else:\n                answer_pred = validation_dataset[row]['context'][pred_char_start:]\n                \n            if answer_pred in validation_dataset[row]['answers']['text']:\n                correct += 1\n\n        else:\n            if (len(validation_dataset[row]['answers']['text']) ==0):\n                correct += 1        \n\n        pred_dict[validation_dataset[row]['id']] = answer_pred\n        na_prob_dict[validation_dataset[row]['id']] = scores[0,0]\n\n        row+=1\n\n\naccuracy = correct/validation_dataset.num_rows\nprint(\"accuracy is: \", accuracy)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T18:14:31.743395Z","iopub.execute_input":"2022-03-13T18:14:31.743659Z","iopub.status.idle":"2022-03-13T18:48:17.418811Z","shell.execute_reply.started":"2022-03-13T18:14:31.743629Z","shell.execute_reply":"2022-03-13T18:48:17.417995Z"},"trusted":true},"execution_count":124,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n100%|██████████| 1779/1779 [33:43<00:00,  1.14s/it]","output_type":"stream"},{"name":"stdout","text":"accuracy is:  0.30852484362920796\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"import json \nwith open(\"pred4.json\", \"w\") as outfile:\n    json.dump(pred_dict, outfile)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T18:59:59.836620Z","iopub.execute_input":"2022-03-13T18:59:59.837413Z","iopub.status.idle":"2022-03-13T18:59:59.874723Z","shell.execute_reply.started":"2022-03-13T18:59:59.837374Z","shell.execute_reply":"2022-03-13T18:59:59.873986Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"import json \nwith open(\"na_prob4.json\", \"w\") as outfile:\n    json.dump(na_prob_dict, outfile)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T19:00:01.385053Z","iopub.execute_input":"2022-03-13T19:00:01.385575Z","iopub.status.idle":"2022-03-13T19:00:01.445002Z","shell.execute_reply.started":"2022-03-13T19:00:01.385538Z","shell.execute_reply":"2022-03-13T19:00:01.444163Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"code","source":"!python evaluation.py \"../input/trivia-squadformat/triviaqa_dev (1).json\" pred4.json --na-prob-file na_prob4.json","metadata":{"execution":{"iopub.status.busy":"2022-03-13T19:00:02.994298Z","iopub.execute_input":"2022-03-13T19:00:02.995090Z","iopub.status.idle":"2022-03-13T19:00:08.168886Z","shell.execute_reply.started":"2022-03-13T19:00:02.994990Z","shell.execute_reply":"2022-03-13T19:00:08.167892Z"},"trusted":true},"execution_count":128,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n{\n  \"exact\": 30.852484362920794,\n  \"f1\": 30.858106683533627,\n  \"total\": 14229,\n  \"HasAns_exact\": 0.0406421459053038,\n  \"HasAns_f1\": 0.04877057508636456,\n  \"HasAns_total\": 9842,\n  \"NoAns_exact\": 99.97720537953043,\n  \"NoAns_f1\": 99.97720537953043,\n  \"NoAns_total\": 4387,\n  \"best_exact\": 30.852484362920794,\n  \"best_exact_thresh\": 0.09740130603313446,\n  \"best_f1\": 30.858106683533627,\n  \"best_f1_thresh\": 0.09740130603313446,\n  \"pr_exact_ap\": 0.00035310544329806957,\n  \"pr_f1_ap\": 0.0004063443578802724,\n  \"pr_oracle_ap\": 67.37022560958722\n}\n","output_type":"stream"}]}]}